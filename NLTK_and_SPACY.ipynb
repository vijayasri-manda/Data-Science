{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO2s4ND+dmKraFs1SIy0xF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijayasri-manda/Data-Science/blob/main/NLTK_and_SPACY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP:**Natural Language Processing\n",
        "NLP is not a language or library it is a field of AI,Which is used for analyzing and understanding **Human language** through python libaries such as **NLTK,SPACY,GENSIM** and etc..\n",
        "\n",
        "\n",
        "# Applications of NLP\n",
        "* spell Checking\n",
        "* keyword Search\n",
        "* Information Extraction\n",
        "* Advertisement Matching\n",
        "* Sentimental Analysis\n",
        "* Speech Recognition\n",
        "* Machine Translation"
      ],
      "metadata": {
        "id": "dPmZzRmckprs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLTK:**Natural Lanaguage Tool Kit\n",
        "It is a python library for performing various natural language processing tasks.\n",
        "\n",
        "**Some Crucial NLP Tasks:**\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lematization\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lDu7XMQRD5O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByXrNwwreOON",
        "outputId": "47b48eca-83b6-439d-ac58-67e0ef4a6074"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords** means commonly used words"
      ],
      "metadata": {
        "id": "bLngiRL_oTZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords=nltk.corpus.stopwords.words('english')\n",
        "nltk_stopwords"
      ],
      "metadata": {
        "id": "xkI63leMebjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb28429-6ae7-471d-991b-7da34dbcdc30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"List of stop words in english:\\n%s\"%list(nltk_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XWIsj-ceiso",
        "outputId": "05bc4cdb-78c2-4c64-ebce-b9bd60c1910a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of stop words in english:\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of stop words\",len(nltk_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqEOdq3wev-l",
        "outputId": "75de81d7-6857-4b3c-f87a-af8a708d51b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of stop words 198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUqOEEyZfZWz",
        "outputId": "2cdd956f-8eae-43c3-b360-ddcb66dad3bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('spanish'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYcu-dT9fHZX",
        "outputId": "474fc413-4f02-48d2-9549-8b15dfddfb46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"A wrapper around a sequence of simple (string) tokens, which is intended to support initial exploration of texts (via the interactive console).\n",
        "Its methods perform a variety of analyses on the text’s contexts (e.g., counting, concordancing, collocation discovery), and display the results. If you wish to write a program which makes use of these analyses, then you should bypass the Text class, and use the appropriate analysis function or class directly instead.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Q7UFupYwfXZ6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNIU-KbTgUDx",
        "outputId": "cb6bad41-c729-402b-cf21-d097fe35c43f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "\n",
        "*   Break a Complex sentence into words\n",
        "*   Understand the importance of each words with respect to the sentence\n",
        "*  Produce a structural description on an input sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YEmwTBxbsunI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**\n",
        "# word_tokenize():\n",
        "This method used to break the given text into words."
      ],
      "metadata": {
        "id": "osiFu5qlzG8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_NLTK=nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "FnPPn1iVgAYW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB1m10eDgSSo",
        "outputId": "c85339f0-7fdb-4f21-d103-9d89f5e63eb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'wrapper',\n",
              " 'around',\n",
              " 'a',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'simple',\n",
              " '(',\n",
              " 'string',\n",
              " ')',\n",
              " 'tokens',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'intended',\n",
              " 'to',\n",
              " 'support',\n",
              " 'initial',\n",
              " 'exploration',\n",
              " 'of',\n",
              " 'texts',\n",
              " '(',\n",
              " 'via',\n",
              " 'the',\n",
              " 'interactive',\n",
              " 'console',\n",
              " ')',\n",
              " '.',\n",
              " 'Its',\n",
              " 'methods',\n",
              " 'perform',\n",
              " 'a',\n",
              " 'variety',\n",
              " 'of',\n",
              " 'analyses',\n",
              " 'on',\n",
              " 'the',\n",
              " 'text',\n",
              " '’',\n",
              " 's',\n",
              " 'contexts',\n",
              " '(',\n",
              " 'e.g.',\n",
              " ',',\n",
              " 'counting',\n",
              " ',',\n",
              " 'concordancing',\n",
              " ',',\n",
              " 'collocation',\n",
              " 'discovery',\n",
              " ')',\n",
              " ',',\n",
              " 'and',\n",
              " 'display',\n",
              " 'the',\n",
              " 'results',\n",
              " '.',\n",
              " 'If',\n",
              " 'you',\n",
              " 'wish',\n",
              " 'to',\n",
              " 'write',\n",
              " 'a',\n",
              " 'program',\n",
              " 'which',\n",
              " 'makes',\n",
              " 'use',\n",
              " 'of',\n",
              " 'these',\n",
              " 'analyses',\n",
              " ',',\n",
              " 'then',\n",
              " 'you',\n",
              " 'should',\n",
              " 'bypass',\n",
              " 'the',\n",
              " 'Text',\n",
              " 'class',\n",
              " ',',\n",
              " 'and',\n",
              " 'use',\n",
              " 'the',\n",
              " 'appropriate',\n",
              " 'analysis',\n",
              " 'function',\n",
              " 'or',\n",
              " 'class',\n",
              " 'directly',\n",
              " 'instead',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_NLTK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo9xCp4ug4R8",
        "outputId": "689304de-71b8-4ff7-9be0-0378db5a7234"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenization**\n",
        "# sent_tokenize():\n",
        "This method used to break the given text into sentences"
      ],
      "metadata": {
        "id": "cgcf5YHXy2G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_NLTK=nltk.sent_tokenize(text)\n",
        "sentences_NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p4CxH52ydmg",
        "outputId": "63839801-7150-4c15-f9d9-a8c07716e9a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A wrapper around a sequence of simple (string) tokens, which is intended to support initial exploration of texts (via the interactive console).',\n",
              " 'Its methods perform a variety of analyses on the text’s contexts (e.g., counting, concordancing, collocation discovery), and display the results.',\n",
              " 'If you wish to write a program which makes use of these analyses, then you should bypass the Text class, and use the appropriate analysis function or class directly instead.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences_NLTK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubWdPPimzTwf",
        "outputId": "4cfd05cd-d8ec-4e73-bc68-7c2fa157f1d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword removal** is a process to remove common words from the document.\n",
        "\n"
      ],
      "metadata": {
        "id": "hviB0yloYYG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove Stopwords\n",
        "filtered_tokens_NLTK=[word for word in tokens_NLTK if word.lower() not in stopwords.words('english')]\n",
        "print(\"Original TOkens:\",tokens_NLTK)\n",
        "print(\"Filtered Tokens:\",filtered_tokens_NLTK)\n",
        "print(\"old text length:\",len(tokens_NLTK))\n",
        "print(\"new text length:\",len(filtered_tokens_NLTK))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LUmI9-fhO5o",
        "outputId": "a0fb616e-6d15-41bd-d2f9-4b2e1cbb5cd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original TOkens: ['A', 'wrapper', 'around', 'a', 'sequence', 'of', 'simple', '(', 'string', ')', 'tokens', ',', 'which', 'is', 'intended', 'to', 'support', 'initial', 'exploration', 'of', 'texts', '(', 'via', 'the', 'interactive', 'console', ')', '.', 'Its', 'methods', 'perform', 'a', 'variety', 'of', 'analyses', 'on', 'the', 'text', '’', 's', 'contexts', '(', 'e.g.', ',', 'counting', ',', 'concordancing', ',', 'collocation', 'discovery', ')', ',', 'and', 'display', 'the', 'results', '.', 'If', 'you', 'wish', 'to', 'write', 'a', 'program', 'which', 'makes', 'use', 'of', 'these', 'analyses', ',', 'then', 'you', 'should', 'bypass', 'the', 'Text', 'class', ',', 'and', 'use', 'the', 'appropriate', 'analysis', 'function', 'or', 'class', 'directly', 'instead', '.']\n",
            "Filtered Tokens: ['wrapper', 'around', 'sequence', 'simple', '(', 'string', ')', 'tokens', ',', 'intended', 'support', 'initial', 'exploration', 'texts', '(', 'via', 'interactive', 'console', ')', '.', 'methods', 'perform', 'variety', 'analyses', 'text', '’', 'contexts', '(', 'e.g.', ',', 'counting', ',', 'concordancing', ',', 'collocation', 'discovery', ')', ',', 'display', 'results', '.', 'wish', 'write', 'program', 'makes', 'use', 'analyses', ',', 'bypass', 'Text', 'class', ',', 'use', 'appropriate', 'analysis', 'function', 'class', 'directly', 'instead', '.']\n",
            "old text length: 90\n",
            "new text length: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s=stopwords.words('tamil')\n",
        "print(list(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpwbJ1mB0Isd",
        "outputId": "f3c08d64-cd11-47cb-ad44-83bddd4d647a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்', 'அதற்கு', 'அதிக', 'அதில்', 'அது', 'அதே', 'அதை', 'அந்த', 'அந்தக்', 'அந்தப்', 'அன்று', 'அல்லது', 'அவன்', 'அவரது', 'அவர்', 'அவர்கள்', 'அவள்', 'அவை', 'ஆகிய', 'ஆகியோர்', 'ஆகும்', 'இங்கு', 'இங்கே', 'இடத்தில்', 'இடம்', 'இதனால்', 'இதனை', 'இதன்', 'இதற்கு', 'இதில்', 'இது', 'இதை', 'இந்த', 'இந்தக்', 'இந்தத்', 'இந்தப்', 'இன்னும்', 'இப்போது', 'இரு', 'இருக்கும்', 'இருந்த', 'இருந்தது', 'இருந்து', 'இவர்', 'இவை', 'உன்', 'உள்ள', 'உள்ளது', 'உள்ளன', 'எந்த', 'என', 'எனக்', 'எனக்கு', 'எனப்படும்', 'எனவும்', 'எனவே', 'எனினும்', 'எனும்', 'என்', 'என்ன', 'என்னும்', 'என்பது', 'என்பதை', 'என்ற', 'என்று', 'என்றும்', 'எல்லாம்', 'ஏன்', 'ஒரு', 'ஒரே', 'ஓர்', 'கொண்ட', 'கொண்டு', 'கொள்ள', 'சற்று', 'சிறு', 'சில', 'சேர்ந்த', 'தனது', 'தன்', 'தவிர', 'தான்', 'நான்', 'நாம்', 'நீ', 'பற்றி', 'பற்றிய', 'பல', 'பலரும்', 'பல்வேறு', 'பின்', 'பின்னர்', 'பிற', 'பிறகு', 'பெரும்', 'பேர்', 'போது', 'போன்ற', 'போல', 'போல்', 'மட்டுமே', 'மட்டும்', 'மற்ற', 'மற்றும்', 'மிக', 'மிகவும்', 'மீது', 'முதல்', 'முறை', 'மேலும்', 'மேல்', 'யார்', 'வந்த', 'வந்து', 'வரும்', 'வரை', 'வரையில்', 'விட', 'விட்டு', 'வேண்டும்', 'வேறு']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s=stopwords.words('german')\n",
        "print(list(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_5l7Zae0Vt8",
        "outputId": "75af0cd6-2bff-4a8d-ae12-b2783452177e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Stemming reduces works to their root by removing suffixes.\n",
        "\n"
      ],
      "metadata": {
        "id": "E1Z5qsmi0oey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"play\"))\n",
        "print(porter.stem(\"playing\"))\n",
        "print(porter.stem(\"plays\"))\n",
        "print(porter.stem(\"played\"))"
      ],
      "metadata": {
        "id": "9TPNSkbe0pvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895431ae-6ced-4f30-df90-944f824c919e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"Communication\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMg8QQ5oC4oa",
        "outputId": "5476d234-2a45-4460-f49a-32295d901963"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"vijayasrireddy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgv7Z0kFC6yi",
        "outputId": "2f04f17d-6b55-4778-fce1-2fbd715de8e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vijayasrireddi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        " It reduces words to their base or root form.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PV2DIuygDIO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "# Download the 'wordnet' corpus\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zveKmEIrEWoU",
        "outputId": "05cc18e4-ba25-468d-af26-eb6c01f5dd57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"plays\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"played\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"play\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"playing\", 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s77_bJ5iEyLO",
        "outputId": "1f46dc86-a880-4038-ab77-e6b9afd3fe3b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"Communication\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtKeCJyyE9-1",
        "outputId": "6173db2c-774b-488c-de0e-3a75d3ff71e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Communication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"vijayasrireddy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXmFPjURFEYk",
        "outputId": "4fa72c18-ba3c-4743-d8d3-a85fa532fdf6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vijayasrireddy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy\n",
        "It is a advanced NLP in Python and used for handling large amounts of data."
      ],
      "metadata": {
        "id": "VBXb0ezBZgeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "sGE1PtZuhh9U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spacy_nlp=spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(\"List of stop words in english:\\n%s\"%list(spacy_nlp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUUV4xi2kN1X",
        "outputId": "a679f102-c4dc-4ef5-ddc2-39e9037e38e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of stop words in english:\n",
            "['themselves', 'more', 'something', 'but', 'must', 'take', 'your', 'being', 'get', 'top', 'six', 'then', 'where', '‘re', 'might', 'move', '’m', 'whereafter', 'whatever', \"'ve\", 'does', 'third', 'make', 'from', 'before', 'several', 'latter', 'off', 'over', 'fifty', 'have', 'may', 'never', 'which', 'down', 'eleven', 'or', 'wherein', 'used', 'meanwhile', 'perhaps', 'an', 'nor', 'the', 'whereby', 'however', 'behind', 'first', 'via', 'both', 'neither', 'side', 'using', 'everyone', 'now', 'whither', 'anyone', 'put', 'yet', 'were', 'beside', 'until', 'hundred', 'various', 'on', 'n‘t', 'afterwards', 'except', 'seems', 'would', 'into', 'became', 'for', 'seemed', 'under', 'everywhere', 'of', 'up', 'somehow', 'through', 'four', 'one', 'formerly', 'thereby', 'no', 'three', 'noone', 'too', 'has', 'we', 'n’t', \"'ll\", 'always', 'without', 'though', 'regarding', 'nine', 'anywhere', 'indeed', 'since', 'two', 'eight', 'above', 'nobody', 'and', 'my', 'are', 'whereupon', 'twelve', 'after', 'this', 'among', 'across', 'between', 're', 'former', 'together', 'forty', 'only', 'beyond', 'who', 'onto', 'they', '‘ve', 'upon', 'its', 'due', 'keep', 'bottom', 'mine', 'hereafter', 'us', 'quite', 'anything', 'well', \"n't\", 'becoming', 'wherever', 'becomes', 'unless', 'last', 'should', 'whole', 'say', 'throughout', 'our', 'elsewhere', 'been', 'amount', 'so', 'was', 'thereafter', 'yourselves', 'every', 'you', 'still', 'therefore', 'anyway', 'myself', 'ever', 'them', 'as', 'to', 'also', 'whom', 'name', 'such', '‘ll', 'most', 'thence', 'latterly', 'herein', 'below', 'hence', 'much', 'show', 'therein', 'someone', 'made', 'their', 'give', 'sixty', 'how', 'doing', 'here', 'please', \"'re\", '‘s', 'seeming', 'per', 'do', 'me', 'she', 'almost', 'although', 'none', \"'m\", 'along', 'within', 'ourselves', 'otherwise', 'enough', 'by', 'ours', 'himself', 'besides', 'whoever', 'cannot', 'seem', 'least', 'in', 'see', 'herself', 'whereas', 'can', 'him', 'when', 'rather', 'had', '’d', 'full', 'ca', 'itself', 'toward', '’re', 'around', 'all', 'even', 'any', 'next', 'his', 'her', 'thereupon', 'with', 'less', 'nowhere', 'there', 'own', 'why', 'while', 'is', 'could', 'thus', 'than', 'out', 'empty', 'will', 'others', 'yourself', 'done', 'front', 'once', 'ten', 'a', 'nevertheless', 'other', 'become', 'am', 'hers', 'already', 'five', 'namely', 'often', 'whence', 'towards', '’s', 'about', 'fifteen', 'it', 'serious', '‘m', \"'s\", 'be', 'if', 'whose', 'anyhow', 'at', 'same', 'i', '‘d', '’ll', 'call', 'few', 'amongst', 'back', 'alone', 'sometimes', 'not', 'some', 'else', 'each', 'hereby', 'very', 'because', 'nothing', 'these', 'twenty', 'again', 'against', 'really', 'go', 'moreover', 'just', 'part', 'he', 'further', 'either', 'that', 'those', 'thru', 'yours', 'what', 'sometime', 'mostly', 'did', '’ve', 'hereupon', 'whenever', 'beforehand', 'whether', 'somewhere', 'during', 'another', \"'d\", 'many', 'everything']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#length of stopwords\n",
        "print(\"length of stop words\",len(spacy_nlp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsYtSUDPjGDs",
        "outputId": "8588d8bc-b5e8-412a-f76a-5d583d0222e1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of stop words 326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "tokens_spacy = [token.text for token in spacy_model(text)]\n",
        "print(tokens_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHmgKNEejVlx",
        "outputId": "d0bfa729-c024-46b0-dbcd-b2c8f70fc756"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'wrapper', 'around', 'a', 'sequence', 'of', 'simple', '(', 'string', ')', 'tokens', ',', 'which', 'is', 'intended', 'to', 'support', 'initial', 'exploration', 'of', 'texts', '(', 'via', 'the', 'interactive', 'console', ')', '.', '\\n', 'Its', 'methods', 'perform', 'a', 'variety', 'of', 'analyses', 'on', 'the', 'text', '’s', 'contexts', '(', 'e.g.', ',', 'counting', ',', 'concordancing', ',', 'collocation', 'discovery', ')', ',', 'and', 'display', 'the', 'results', '.', 'If', 'you', 'wish', 'to', 'write', 'a', 'program', 'which', 'makes', 'use', 'of', 'these', 'analyses', ',', 'then', 'you', 'should', 'bypass', 'the', 'Text', 'class', ',', 'and', 'use', 'the', 'appropriate', 'analysis', 'function', 'or', 'class', 'directly', 'instead', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8fI2qOJj4kc",
        "outputId": "7933b3da-9a9c-4684-a5d1-0df46ed20cdd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens_spacy=[word for word in tokens_spacy if word.lower() not in spacy_nlp]\n",
        "print(\"Original TOkens:\",tokens_spacy)\n",
        "print(\"Filtered Tokens:\",filtered_tokens_spacy)\n",
        "print(\"old text length:\",len(tokens_spacy))\n",
        "print(\"new text length:\",len(filtered_tokens_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h59rs2zEkYpz",
        "outputId": "6473ec96-a945-45bf-98d4-ba3076af9c90"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original TOkens: ['A', 'wrapper', 'around', 'a', 'sequence', 'of', 'simple', '(', 'string', ')', 'tokens', ',', 'which', 'is', 'intended', 'to', 'support', 'initial', 'exploration', 'of', 'texts', '(', 'via', 'the', 'interactive', 'console', ')', '.', '\\n', 'Its', 'methods', 'perform', 'a', 'variety', 'of', 'analyses', 'on', 'the', 'text', '’s', 'contexts', '(', 'e.g.', ',', 'counting', ',', 'concordancing', ',', 'collocation', 'discovery', ')', ',', 'and', 'display', 'the', 'results', '.', 'If', 'you', 'wish', 'to', 'write', 'a', 'program', 'which', 'makes', 'use', 'of', 'these', 'analyses', ',', 'then', 'you', 'should', 'bypass', 'the', 'Text', 'class', ',', 'and', 'use', 'the', 'appropriate', 'analysis', 'function', 'or', 'class', 'directly', 'instead', '.', '\\n']\n",
            "Filtered Tokens: ['wrapper', 'sequence', 'simple', '(', 'string', ')', 'tokens', ',', 'intended', 'support', 'initial', 'exploration', 'texts', '(', 'interactive', 'console', ')', '.', '\\n', 'methods', 'perform', 'variety', 'analyses', 'text', 'contexts', '(', 'e.g.', ',', 'counting', ',', 'concordancing', ',', 'collocation', 'discovery', ')', ',', 'display', 'results', '.', 'wish', 'write', 'program', 'makes', 'use', 'analyses', ',', 'bypass', 'Text', 'class', ',', 'use', 'appropriate', 'analysis', 'function', 'class', 'directly', 'instead', '.', '\\n']\n",
            "old text length: 91\n",
            "new text length: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens_spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9FLV5frkfs-",
        "outputId": "429d8a5d-6105-4ecc-db90-f864131c34b1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wrapper',\n",
              " 'sequence',\n",
              " 'simple',\n",
              " '(',\n",
              " 'string',\n",
              " ')',\n",
              " 'tokens',\n",
              " ',',\n",
              " 'intended',\n",
              " 'support',\n",
              " 'initial',\n",
              " 'exploration',\n",
              " 'texts',\n",
              " '(',\n",
              " 'interactive',\n",
              " 'console',\n",
              " ')',\n",
              " '.',\n",
              " '\\n',\n",
              " 'methods',\n",
              " 'perform',\n",
              " 'variety',\n",
              " 'analyses',\n",
              " 'text',\n",
              " 'contexts',\n",
              " '(',\n",
              " 'e.g.',\n",
              " ',',\n",
              " 'counting',\n",
              " ',',\n",
              " 'concordancing',\n",
              " ',',\n",
              " 'collocation',\n",
              " 'discovery',\n",
              " ')',\n",
              " ',',\n",
              " 'display',\n",
              " 'results',\n",
              " '.',\n",
              " 'wish',\n",
              " 'write',\n",
              " 'program',\n",
              " 'makes',\n",
              " 'use',\n",
              " 'analyses',\n",
              " ',',\n",
              " 'bypass',\n",
              " 'Text',\n",
              " 'class',\n",
              " ',',\n",
              " 'use',\n",
              " 'appropriate',\n",
              " 'analysis',\n",
              " 'function',\n",
              " 'class',\n",
              " 'directly',\n",
              " 'instead',\n",
              " '.',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"existing length of the stopwords in nltk library:\",len(nltk_stopwords))\n",
        "print(\"existing length of the stopwords in spacy library:\",len(spacy_nlp))"
      ],
      "metadata": {
        "id": "4sgkYVVGm6u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105eeb92-7872-4ed0-a9d7-9c34b95ce4d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "existing length of the stopwords in nltk library: 198\n",
            "existing length of the stopwords in spacy library: 326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords.append('example1')\n",
        "len(nltk_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD4uynuHK1oS",
        "outputId": "948ddb22-c6be-4447-cc07-74561d73f407"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords.extend(['example2','example3'])\n",
        "len(nltk_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_0BL2JhLENr",
        "outputId": "a54e2d9e-274a-4b47-8c51-6a7d473813da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUKnMSEALJPa",
        "outputId": "4f7302c8-bf67-44f8-9474-6d1a2498311b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\",\n",
              " 'example1',\n",
              " 'example2',\n",
              " 'example3']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords.remove('example1')\n",
        "len(nltk_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjVKtmmuLOz5",
        "outputId": "45f1b567-742d-4391-fbb9-70c1fc5a0ed8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T0mPXk5LSol",
        "outputId": "d4d641b8-941e-4b8b-99cd-9ce678ecc9cc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\",\n",
              " 'example2',\n",
              " 'example3']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between NLTK and Spacy\n",
        "**NLTK**\n",
        "* NLTK is mainly a string processing library.\n",
        "\n",
        "* Provides access to many algorithms.If you care about specific algorithms and customizations go with NLTK.\n",
        "* NLTK is old library.User community as active as Spacy.\n",
        "* 198 Stopwords.\n",
        "\n",
        "\n",
        "**SPACY**\n",
        "* Spacy is Object oriented.\n",
        "* Provides most efficient NLP algortihm for a given task.Hence if you care about the end result,go with Spacy.\n",
        "* Spacy is new library and has a very active user community.\n",
        "* 326 Stopwords."
      ],
      "metadata": {
        "id": "GKNABBZ0FA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "K-O1tfT-Nieh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"Vijaya sri Manda from Seshadri Rao gudlavalleru Engineering College.Currently pursuing 3rd year btech in Artificial Intelligence and Data science\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltN1qJJ6FEs3",
        "outputId": "59c5f296-d53f-4ef5-fa4e-5a27e261c43a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vijaya sri Manda from Seshadri Rao gudlavalleru Engineering College.\n",
            "Currently pursuing 3rd year btech in Artificial Intelligence and Data science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "  for word in sentence:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR5FFojtFqSD",
        "outputId": "5595270b-aa75-4091-d9b8-2f0381336ec1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vijaya\n",
            "sri\n",
            "Manda\n",
            "from\n",
            "Seshadri\n",
            "Rao\n",
            "gudlavalleru\n",
            "Engineering\n",
            "College\n",
            ".\n",
            "Currently\n",
            "pursuing\n",
            "3rd\n",
            "year\n",
            "btech\n",
            "in\n",
            "Artificial\n",
            "Intelligence\n",
            "and\n",
            "Data\n",
            "science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "B4OgPFI4F2XN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(\"Vijaya sri Manda from Seshadri Rao gudlavalleru Engineering College.Currently pursuing 3rd year btech in Artificial Intelligence and Data science\")"
      ],
      "metadata": {
        "id": "ABydMsgRF-SA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90adb74-e4fe-4fe7-9494-494162f0005e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vijaya sri Manda from Seshadri Rao gudlavalleru Engineering College.Currently pursuing 3rd year btech in Artificial Intelligence and Data science']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Vijaya sri Manda from Seshadri Rao gudlavalleru Engineering College.Currently pursuing 3rd year btech in Artificial Intelligence and Data science\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR62E8XYGKPm",
        "outputId": "90d8d32b-867f-429b-d3ca-d1cceae7cc4d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vijaya',\n",
              " 'sri',\n",
              " 'Manda',\n",
              " 'from',\n",
              " 'Seshadri',\n",
              " 'Rao',\n",
              " 'gudlavalleru',\n",
              " 'Engineering',\n",
              " 'College.Currently',\n",
              " 'pursuing',\n",
              " '3rd',\n",
              " 'year',\n",
              " 'btech',\n",
              " 'in',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'and',\n",
              " 'Data',\n",
              " 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customation of stopwords\n",
        "text='''The community of volunteers behind Wikipedia is the most important and unique element of Wikipedia’s success. For nearly 25 years, Wikipedia editors have researched, deliberated, discussed, built consensus, and collaboratively written the largest encyclopedia humankind has ever seen. Their care and commitment to reliable encyclopedic knowledge is something AI cannot replace.\n",
        "\n",
        "That is why our new AI strategy doubles down on the volunteers behind Wikipedia.\n",
        "\n",
        "We will use AI to build features that remove technical barriers to allow the humans at the core of Wikipedia to spend their valuable time on what they want to accomplish, and not on how to technically achieve it. Our investments will be focused on specific areas where generative AI excels, all in the service of creating unique opportunities that will boost Wikipedia’s volunteers:\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "C0PRXFFrGnrP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords=['Wikipedia','AI','the','to','is']"
      ],
      "metadata": {
        "id": "J33Yosy6MJiG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "Zq5PjnfbMZIY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCklhOpvMtea",
        "outputId": "6239b3f6-1ebd-4603-923e-1124a1bbeac5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'community', 'of', 'volunteers', 'behind', 'Wikipedia', 'is', 'the', 'most', 'important', 'and', 'unique', 'element', 'of', 'Wikipedia', '’', 's', 'success', '.', 'For', 'nearly', '25', 'years', ',', 'Wikipedia', 'editors', 'have', 'researched', ',', 'deliberated', ',', 'discussed', ',', 'built', 'consensus', ',', 'and', 'collaboratively', 'written', 'the', 'largest', 'encyclopedia', 'humankind', 'has', 'ever', 'seen', '.', 'Their', 'care', 'and', 'commitment', 'to', 'reliable', 'encyclopedic', 'knowledge', 'is', 'something', 'AI', 'can', 'not', 'replace', '.', 'That', 'is', 'why', 'our', 'new', 'AI', 'strategy', 'doubles', 'down', 'on', 'the', 'volunteers', 'behind', 'Wikipedia', '.', 'We', 'will', 'use', 'AI', 'to', 'build', 'features', 'that', 'remove', 'technical', 'barriers', 'to', 'allow', 'the', 'humans', 'at', 'the', 'core', 'of', 'Wikipedia', 'to', 'spend', 'their', 'valuable', 'time', 'on', 'what', 'they', 'want', 'to', 'accomplish', ',', 'and', 'not', 'on', 'how', 'to', 'technically', 'achieve', 'it', '.', 'Our', 'investments', 'will', 'be', 'focused', 'on', 'specific', 'areas', 'where', 'generative', 'AI', 'excels', ',', 'all', 'in', 'the', 'service', 'of', 'creating', 'unique', 'opportunities', 'that', 'will', 'boost', 'Wikipedia', '’', 's', 'volunteers', ':']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens=[word for word in tokens if word.lower() not in custom_stopwords]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liA6Fc_LMxnY",
        "outputId": "00b3041e-277b-4889-8d4c-316ad5b32846"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['community', 'of', 'volunteers', 'behind', 'Wikipedia', 'most', 'important', 'and', 'unique', 'element', 'of', 'Wikipedia', '’', 's', 'success', '.', 'For', 'nearly', '25', 'years', ',', 'Wikipedia', 'editors', 'have', 'researched', ',', 'deliberated', ',', 'discussed', ',', 'built', 'consensus', ',', 'and', 'collaboratively', 'written', 'largest', 'encyclopedia', 'humankind', 'has', 'ever', 'seen', '.', 'Their', 'care', 'and', 'commitment', 'reliable', 'encyclopedic', 'knowledge', 'something', 'AI', 'can', 'not', 'replace', '.', 'That', 'why', 'our', 'new', 'AI', 'strategy', 'doubles', 'down', 'on', 'volunteers', 'behind', 'Wikipedia', '.', 'We', 'will', 'use', 'AI', 'build', 'features', 'that', 'remove', 'technical', 'barriers', 'allow', 'humans', 'at', 'core', 'of', 'Wikipedia', 'spend', 'their', 'valuable', 'time', 'on', 'what', 'they', 'want', 'accomplish', ',', 'and', 'not', 'on', 'how', 'technically', 'achieve', 'it', '.', 'Our', 'investments', 'will', 'be', 'focused', 'on', 'specific', 'areas', 'where', 'generative', 'AI', 'excels', ',', 'all', 'in', 'service', 'of', 'creating', 'unique', 'opportunities', 'that', 'will', 'boost', 'Wikipedia', '’', 's', 'volunteers', ':']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-aeQv-RNGez",
        "outputId": "9d10d9ae-f71e-4bb5-b83a-d8675f4b0240"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMIkXJ5ZM759",
        "outputId": "bf57e41d-37a9-4bec-9859-036eaa5abb5a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}